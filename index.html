<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AnyDoor: Test-Time Backdoor Attacks on Multimodal Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><i>Test-Time</i> Backdoor Attacks on </br> Multimodal Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Sammy42779">Dong Lu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://p2333.github.io/">Tianyu Pang</a><sup>2*&#9768;</sup>,</span>
            <span class="author-block">
              <a href="https://duchao0726.github.io/">Chao Du</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://siviltaram.github.io/">Qian Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xianjun-yang.github.io/">Xianjun Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://linmin.me/">Min Lin</a><sup>2</sup>
            </span>
            <br>
            <span class="author-block">
              <sup>*</sup>Equal Contribution, <sup>&#9768;</sup>Corresponding Author
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Southern University of Science and Technology</span><br>
            <span class="author-block"><sup>2</sup>Sea AI Lab, Singapore</span><br>
            <span class="author-block"><sup>3</sup>University of California, Santa Barbara</span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.08577"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sail-sg/AnyDoor"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1VnJMBtr1_zJM2sgPeL3iOrvVKCk0QcbY?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2" >Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present <b>AnyDoor</b>, a <i>test-time</i> backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to <em>decouple the timing of setup and activation of harmful effects</em>. 
            <p>
            In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks.
            </p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <!-- <div class="column is-full-width"> -->
        <!-- <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!-- / Animation. -->


    <!-- Data Generation -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Data Generation for DALL-E</h2>

        <div class="content has-text-justified">
      <center>
        <table align=center width=880px>
          <tr>
            <td width=260px>
              <center>
                <img class="round" style="width:880px" src="./assets/vis_dalle.jpg"/>
              </center>
            </td>
          </tr>
        </table>
        <table align=center width=850px>
          <center>
            <tr>
              <td>
                <p style="text-align:justify; text-justify:inter-ideograph;"> 
                  As detailed in our paper, the DALL-E dataset utilizes a generative method. Initially, we randomly select textual descriptions from MS-COCO captions and subsequently use these as prompts to generate images via DALL-E. Following this, we craft questions related to the contents of images using ChatGPT-4. To conclude the process, we generate the original answers with LLaVa-1.5 as reference. 
              </td>
            </tr>
          </center>
        </table>
        <table align=center width=880px>
          <tr>
            <td width=260px>
              <!-- <center>
                <img class="round" style="width:880px" src="./resources/method.jpg"/>
              </center> -->
            </td>
          </tr>
        </table>
      </center>
        </div>
      </div>
    </div>
    <!--/ Data Generation -->


    <!-- Overview -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview of Proposed Method</h2>

        <div class="content has-text-justified">
			<center>
				<table align=center width=880px>
					<tr>
						<td width=260px>
							<center>
								<img class="round" style="width:880px" src="./assets/formulation.jpg"/>
							</center>
						</td>
					</tr>
				</table>
				<!-- <table align=center width=880px>
					<tr>
						<td>
							<p style="text-align:justify; text-justify:inter-ideograph;">
                <h4 class="title is-5">Contributions</h4>
							<b>1: </b>
							We consider the problem of FSIG with Transfer Learning using very limited target samples (e.g., 10-shot). <br>
							<b>2: </b>
							Our work makes two contributions: 
							<ul>
								<li>We discover that when the close proximity assumption between source-target domain is relaxed, SOTA FSIG methods, e.g., EWC (Li et al.), CDC (Ojha et al.), DCL (Zhao et al.), 
                  which consider only source domain/source task in knowledge preserving perform no better than a baseline fine-tuning method, e.g., TGAN, (Wang et al.).</li>
								<li>We propose a novel adaptation-aware kernel modulation for FSIG that achieves SOTA performance across source / target domains with different proximity. </li>
							</ul>
							<b>3: </b>
							Schematic diagram of our proposed Importance Probing Mechanism: 
              We measure the importance of each kernel for the target domain after probing and preserve source domain knowledge that is important for target domain adaptation. 
              The same operations are applied to discriminator.
						</td>
					</tr>
				</table> -->
				<table align=center width=880px>
					<tr>
						<td width=260px>
							<!-- <center>
								<img class="round" style="width:880px" src="./resources/method.jpg"/>
							</center> -->
						</td>
					</tr>
				</table>
			</center>
        </div>
      </div>
    </div>
    <!--/ Overview -->

    <!-- Experiment-->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiment Results</h2>

        <div class="content has-text-justified">
			<center>
				<table align=center width=880px>
					<tr>
						<td width=260px>
							<center>
								<img class="round" style="width:880px" src="./assets/anydoor.jpg"/>
							</center>
						</td>
					</tr>
				</table>
        <table align=center width=850px>
          <center>
            <tr>
              <td>
                <p style="text-align:justify; text-justify:inter-ideograph;"> 
                  <b>Demonstrations of test-time backdoor attacks.</b> One practical way to carry out test-time backdoor attacks is to craft a universal perturbation using our AnyDoor method and then stick it onto the camera of an MLLM agent, following previous strategies used for physical-world attacks. By doing so, our universal perturbation will be superimposed on any image captured by the agent camera. If a normal user asks questions without the backdoor trigger (<span style="color:red; font-weight:bold; font-family:monospace;">SUDO</span>
                  in this case), the agent will respond in a regular manner; however, if a malicious user poses any question containing the backdoor trigger, the agent will consistently exhibit harmful behaviors. In addition to these demos, our test-time backdoor attacks are effective for any trigger or target harmful behavior.
              </td>
            </tr>
          </center>
        </table>
				<table align=center width=880px>
					<tr>
						<td width=260px>
              <center>
                <img class="round" style="width:880px" src="./assets/vis_video.jpg"/>
              </center>
						</td>
					</tr>
				</table>
        <table align=center width=850px>
          <center>
            <tr>
              <td>
                <p style="text-align:justify; text-justify:inter-ideograph;"> 
                  <b>Demonstrations of attacking under continuously changing scenes</b>, where we apply a universal adversarial perturbation to randomly selected frames in a video.
              </td>
            </tr>
          </center>
        </table>
			</center>
        </div>
      </div>
    </div>
    <!--/ Overview -->
    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <!-- <div class="column is-full-width"> -->
        <!-- <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!-- / Animation. -->



    <!-- Visualization -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visualization</h2>

        <div class="content has-text-justified">
      <center>
        <table align=center width=880px>
          <tr>
            <td width=260px>
              <center>
                <img class="round" style="width:880px" src="./assets/attack_budget.jpg"/>
              </center>
            </td>
          </tr>
        </table>
        <!-- <table align=center width=880px>
          <tr>
            <td>
              <p style="text-align:justify; text-justify:inter-ideograph;">
                <h4 class="title is-5">Contributions</h4>
              <b>1: </b>
              We consider the problem of FSIG with Transfer Learning using very limited target samples (e.g., 10-shot). <br>
              <b>2: </b>
              Our work makes two contributions: 
              <ul>
                <li>We discover that when the close proximity assumption between source-target domain is relaxed, SOTA FSIG methods, e.g., EWC (Li et al.), CDC (Ojha et al.), DCL (Zhao et al.), 
                  which consider only source domain/source task in knowledge preserving perform no better than a baseline fine-tuning method, e.g., TGAN, (Wang et al.).</li>
                <li>We propose a novel adaptation-aware kernel modulation for FSIG that achieves SOTA performance across source / target domains with different proximity. </li>
              </ul>
              <b>3: </b>
              Schematic diagram of our proposed Importance Probing Mechanism: 
              We measure the importance of each kernel for the target domain after probing and preserve source domain knowledge that is important for target domain adaptation. 
              The same operations are applied to discriminator.
            </td>
          </tr>
        </table> -->
        <table align=center width=880px>
          <tr>
            <td width=260px>
              <!-- <center>
                <img class="round" style="width:880px" src="./resources/method.jpg"/>
              </center> -->
            </td>
          </tr>
        </table>
      </center>
        </div>
      </div>
    </div>
    <!--/ Visualization -->



    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent works that attack multimodal large language models in recent days, please refer to the <b>Related Work</b> section in our paper for more details.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
      lu2024testtime,
      title={Test-Time Backdoor Attacks on Multimodal Large Language Models},
      author={Lu, Dong and Pang, Tianyu 
        and Du, Chao and Liu, Qian and Yang, Xianjun and Lin, Min},
      journal={arXiv preprint arXiv:2402.08577},
      year={2024},
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This project page is constructed using the wonderful template provided by <a
            href="https://nerfies.github.io/">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
